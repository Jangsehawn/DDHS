{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "2YtrY2X0oazv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self, input_dim, latent_dim):\n",
        "      super().__init__()\n",
        "      self.encoder = nn.Sequential(\n",
        "           nn.Linear(input_dim, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, latent_dim)\n",
        "        )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(latent_dim, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 256),\n",
        "          nn.ReLU(),            \n",
        "          nn.Linear(256, input_dim)\n",
        "        )\n",
        "      self.centers = nn.Parameter(torch.randn(2, latent_dim))\n",
        "    \n",
        "  def forward(self, x):\n",
        "      encoded = self.encoder(x)\n",
        "      decoded = self.decoder(encoded)        \n",
        "      return encoded, decoded\n",
        "    \n",
        "  def get_center_loss(self, encoded, target):\n",
        "      batch_size = encoded.size(0)\n",
        "      target = target.reshape(-1, 1) #   view\n",
        "      centers_batch = self.centers.gather(0, target.to(torch.int64).repeat(1, encoded.size(1))) \n",
        "      center_loss = (encoded - centers_batch).pow(2).sum() / batch_size\n",
        "      return center_loss\n",
        "  \n",
        "\n",
        "\n",
        "# imbalanced에 data level로 해결하는 모델\n",
        "class DDHS:\n",
        "  #데이터를 KDE로 가우시안 분포를 이용하여 중간 %를 추출하는 함수\n",
        "  # start, last에 퍼센트를 입력\n",
        "\n",
        "  def extract_middle_percent(self,data, start, last):\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(data_scaled)\n",
        "    log_prob = kde.score_samples(data_scaled)\n",
        "    prob = np.exp(log_prob)\n",
        "    threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
        "    mask = np.logical_and(prob >= threshold_low, prob <= threshold_high)\n",
        "    data_middle = data[mask]\n",
        "    if len(data_middle) > 0 :\n",
        "      return data_middle\n",
        "    else:  \n",
        "      print(\"No middle 50% found, returning original data\")\n",
        "      return np.array([])\n",
        "\n",
        "  #  각 feature 안의 값을 복원추출하는 함수\n",
        "\n",
        "  def reconstruct_features(self,data):\n",
        "    mean = data.mean(axis=0)\n",
        "    std = data.std(axis=0)\n",
        "    reconstructed = np.random.randn(*data.shape) * std + mean\n",
        "    return reconstructed\n",
        "\n",
        "  # synthetic sample을 생성하는 함수\n",
        "  # 라벨과과 데이터를 하나의 데이터프레임으로 출력\n",
        "  # small class / large class의 비율 = ratio \n",
        "\n",
        "  def generate_synthetic_sample(self,X,Y, ratio):\n",
        "\n",
        "    data = pd.concat([X,Y],axis=1)\n",
        "\n",
        "    # small class\n",
        "    data_A = data[data[Y.columns[0]]== Y.value_counts().idxmin()[0]  ].loc[:, data.columns != Y.columns[0]].astype(float).values\n",
        "\n",
        "    # large class \n",
        "    data_B = data[data[Y.columns[0]]== Y.value_counts().idxmax()[0] ].loc[:, data.columns != Y.columns[0]].astype(float).values\n",
        "\n",
        "\n",
        "    # autoencoder를 사용하여 잠재 변수를 추출\n",
        "    with torch.no_grad():\n",
        "        encoded_A, _ = self.model(torch.tensor(data_A).float())\n",
        "        encoded_B, _ = self.model(torch.tensor(data_B).float())\n",
        "\n",
        "    # Majority : Q2 보다 Density가 큰 샘플을 Keep→ 50% 샘플을 Classifier 의 Train 데이터로 활용\n",
        "    # Minority : Q3 을 사용해서 더 높은 기준을 설정 → 75% 샘플을 Classifier 의 Train 데이터로 활용 → 25% 샘플을 Subsequence 생성 과정의 샘플로 활용\n",
        "  \n",
        "    encoded_A_middle = self.extract_middle_percent(encoded_A.cpu().numpy(),12.5, 87.5) # 최종 합치기\n",
        "\n",
        "    encoded_B_middle = self.extract_middle_percent(encoded_B.cpu().numpy(),25,75)# 최종 합치기\n",
        "\n",
        "    # 중간 25%의 잠재 변수로부터 feature를 복원추출 -> 추가 확인인\n",
        "    reconstructed_features = self.reconstruct_features(self.extract_middle_percent(encoded_A.cpu().numpy(),37.5, 62.5))\n",
        "\n",
        "    # 임의의 위치에 synthetic sample 생성\n",
        "    center_A = np.mean(encoded_A.cpu().numpy(), axis=0, dtype=np.float64, out=None)\n",
        "\n",
        "    center_B = np.mean(encoded_B.numpy(), axis=0, dtype=np.float64, out=None) \n",
        "\n",
        "    radius_A = np.max(np.linalg.norm(encoded_A.cpu().numpy() - center_A, axis=1))\n",
        "\n",
        "    #encoded_B[Y.columns[0]] = 1 \n",
        "    synthetic_sample = pd.DataFrame() # 최종 합치기\n",
        "   \n",
        "    while (len(synthetic_sample)-len(encoded_B))/len(encoded_B) >= ratio :\n",
        "        z = np.random.randn(latent_dim)\n",
        "        if np.linalg.norm(z - center_A) < np.linalg.norm(z - center_B) and np.linalg.norm(z - center_A) < radius_A:\n",
        "            synthetic_sample.append(z, ignore_index=True)\n",
        "\n",
        "    # 최종 출력할 데이터 \n",
        "    encoded_B_middle = pd.DataFrame(encoded_B_middle)\n",
        "    encoded_B_middle['label'] = Y.value_counts().idxmin()[0]\n",
        "\n",
        "    encoded_A_middle = pd.DataFrame(encoded_A_middle)\n",
        "    encoded_A_middle['label'] = Y.value_counts().idxmax()[0] \n",
        "\n",
        "    synthetic_sample['label'] = Y.value_counts().idxmax()[0] \n",
        "\n",
        "    ouput = pd.concat([encoded_B_middle,encoded_A_middle,synthetic_sample ] )\n",
        "\n",
        "    return ouput.loc[:, ouput.columns != 'label'],ouput['label']\n",
        "\n",
        "  def fit(self,X,Y,lr,num_epochs, ratio):\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    input_dim = len(X.columns) # 데이터의 차원\n",
        "    latent_dim = len(X.columns) # 잠재 변수의 차원\n",
        "\n",
        "\n",
        "    #lr = 1e-3 # 학습률\n",
        "    #num_epochs = 50 # 학습 에폭 수\n",
        "    \n",
        "    self.model = Autoencoder(input_dim, latent_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      x = torch.tensor(X.to_numpy()).float()#.to(device)\n",
        "      y = torch.tensor(Y[Y.columns[0]].to_numpy()).float().to(device)\n",
        "\n",
        "      encoded, decoded = self.model(x)\n",
        "      reconstruction_loss = F.mse_loss(decoded, x)\n",
        "      center_loss = self.model.get_center_loss(encoded, y)\n",
        "\n",
        "      loss = reconstruction_loss + center_loss\n",
        "      cross_entropy_loss = F.cross_entropy(decoded, y.long()) # y를 long 형으로 요구\n",
        "      loss += cross_entropy_loss\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    \n",
        "    synthetic_X, synthetic_Y = self.generate_synthetic_sample(X,Y, ratio)\n",
        "    return synthetic_X, synthetic_Y\n",
        "\n",
        "\n",
        "  def __init__(self):\n",
        "    self.result = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "magic = pd.read_csv('/content/drive/MyDrive/DDHS/magic04.data')\n",
        "model = DDHS()"
      ],
      "metadata": {
        "id": "ast_WSrjvuoC"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magic['g'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KahS-E6fNZeZ",
        "outputId": "21c56100-93a4-4e52-f228-9401467c6224"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "g    12331\n",
              "h     6688\n",
              "Name: g, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def digit_trans(x):\n",
        "  if x == 'g':\n",
        "    return 0\n",
        "  else:\n",
        "    return 1  \n",
        "magic['label']=magic['g'].apply(digit_trans)"
      ],
      "metadata": {
        "id": "h5lxtiu80Mrh"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magic[['label']].columns[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "UCkJOAX1ZpA7",
        "outputId": "7b7b9284-b7a7-4e58-b797-2848cdbd3533"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'label'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "magic[['label']].value_counts().idxmin()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QodrJyPkCyLp",
        "outputId": "0667d4a8-acce-42f1-9c80-14c48795bb00"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "magic2= magic.drop('g',axis=1)\n",
        "magic3 = magic2.drop('label',axis=1)"
      ],
      "metadata": {
        "id": "vAbacsFDDN5y"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(magic3,magic[['label']], 1e-3, 50,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKPkgGbp0MpC",
        "outputId": "ffc4a33d-a08e-4b6a-b405-a88cb3a486af"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(              0          1         2          3          4          5  \\\n",
              " 0      5.355979  -0.887720  4.541102   9.841983  12.506713  -4.606334   \n",
              " 1      5.341043  -2.410148  4.270937   8.797510  10.695181  -5.170622   \n",
              " 2      2.470552  -0.513057  1.787970   4.505671   6.341879  -1.997583   \n",
              " 3      7.936007  -4.786698  5.179059  11.967289  14.986421  -7.474792   \n",
              " 4      7.489469  -4.904233  1.906083   5.128616   5.260751  -5.592462   \n",
              " ...         ...        ...       ...        ...        ...        ...   \n",
              " 5011  10.863337  -6.895259  3.176227   3.892241   4.875031  -9.118051   \n",
              " 5012   2.723989  -0.980387  1.897113   4.063401   5.136490  -2.295637   \n",
              " 5013   7.352121  -1.871160  4.490939   6.575963  10.832455  -6.443671   \n",
              " 5014   7.655222  -3.092923  3.163297   9.398839  11.834931  -6.198069   \n",
              " 5015  23.298279 -19.666574 -2.194257  -2.419691  -1.307037 -16.106039   \n",
              " \n",
              "              6         7          8          9  \n",
              " 0    -1.999897  6.879706   8.466780  -5.326552  \n",
              " 1    -1.490843  5.999754   8.583994  -5.345051  \n",
              " 2    -0.362119  3.840234   4.241322  -1.555991  \n",
              " 3    -1.260901  8.486393  12.809669  -7.600717  \n",
              " 4    -1.024700  2.716087   6.462693  -5.577424  \n",
              " ...        ...       ...        ...        ...  \n",
              " 5011 -1.452955  2.993960   6.713545  -7.962163  \n",
              " 5012 -0.781165  2.691191   4.123037  -2.725135  \n",
              " 5013 -1.518683  5.820093   8.822443  -6.481804  \n",
              " 5014 -0.904444  7.006186  10.999298  -6.333679  \n",
              " 5015  1.933132 -0.859573   5.886294 -13.987865  \n",
              " \n",
              " [11181 rows x 10 columns],\n",
              " 0       1\n",
              " 1       1\n",
              " 2       1\n",
              " 3       1\n",
              " 4       1\n",
              "        ..\n",
              " 5011    0\n",
              " 5012    0\n",
              " 5013    0\n",
              " 5014    0\n",
              " 5015    0\n",
              " Name: label, Length: 11181, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    }
  ]
}